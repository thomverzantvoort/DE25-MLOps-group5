{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3bf0613-d6ec-4cf4-87e5-062fd3bd3a82",
   "metadata": {},
   "source": [
    "### Installation\n",
    "Install the packages required for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1d825-84cc-43ac-9fe2-f204d77f0962",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the packages\n",
    "! pip3 install --user --no-cache-dir --upgrade \"kfp>2\" \"google-cloud-pipeline-components>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bc6a21-604f-4a52-b904-e3bb18a61b2f",
   "metadata": {},
   "source": [
    "## Restart the kernel\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dad0c4-c173-46b8-bf99-d6e8efc35316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2207b06-771f-4dbb-a713-90c50745c0ea",
   "metadata": {},
   "source": [
    "## Check the versions of the packages you installed. The KFP SDK version should be >2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5b60838-e5a2-41cd-ae93-43925343fba5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.14.4\n",
      "google-cloud-aiplatform==1.118.0\n",
      "google_cloud_pipeline_components version: 2.21.0\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform\n",
    "! python3 -c \"import google_cloud_pipeline_components; print('google_cloud_pipeline_components version: {}'.format(google_cloud_pipeline_components.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f0bcff2-3ffb-4e51-b852-511cb10ad0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import typing\n",
    "from typing import Dict\n",
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component, \n",
    "                        OutputPath, \n",
    "                        InputPath)\n",
    "import google.cloud.aiplatform as aip\n",
    "from google_cloud_pipeline_components.types import artifact_types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afffb0-449b-4669-807a-793f526277fe",
   "metadata": {},
   "source": [
    "#### Project and Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf6aad4-f675-47aa-820b-14daa796d89f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "PROJECT_ID = \"agile-producer-471907-s7\"\n",
    "# The region that this pipeline runs in\n",
    "REGION = \"us-central1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "PIPELINE_ROOT = \"gs://temp_spotify_2038050\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ef88-cd95-4304-b6e0-143b718c44aa",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e847a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"pandas\", \"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_data(project_id: str, bucket: str, file_name: str, dataset: Output[Dataset]):\n",
    "    '''Download raw data from GCS'''\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(dataset.path + \".csv\")\n",
    "    logging.info(f'Downloaded {file_name} from gs://{bucket}/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288bd9ec",
   "metadata": {},
   "source": [
    "#### Pipeline Component: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6620acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def preprocess_and_split(\n",
    "    raw_data: Input[Dataset],\n",
    "    train_data: Output[Dataset],\n",
    "    test_data: Output[Dataset],\n",
    "    scaler_out: Output[Model]\n",
    "):\n",
    "    '''Preprocess data and split into train/test sets'''\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import joblib\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # 1. Load raw data\n",
    "    df_raw = pd.read_csv(raw_data.path + \".csv\")\n",
    "    logging.info(f\"Raw data loaded: {df_raw.shape}\")\n",
    "    \n",
    "    # 2. Drop unnecessary columns\n",
    "    drop_cols = [\"user_id\", \"offline_listening\"]\n",
    "    df_model = df_raw.drop(columns=[c for c in drop_cols if c in df_raw.columns])\n",
    "    \n",
    "    # 3. Select only numerical features\n",
    "    selected_features = [\n",
    "        \"age\",\n",
    "        \"listening_time\",\n",
    "        \"songs_played_per_day\",\n",
    "        \"skip_rate\",\n",
    "        \"ads_listened_per_week\"\n",
    "    ]\n",
    "    target_col = \"is_churned\"\n",
    "    \n",
    "    X = df_model[selected_features]\n",
    "    y = df_model[target_col]\n",
    "    \n",
    "    # 4. Train/Test Split (80/20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    logging.info(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "    \n",
    "    # 5. Fit scaler on training data only\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=selected_features\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=selected_features\n",
    "    )\n",
    "    \n",
    "    # 6. Save train data\n",
    "    train_df = X_train_scaled.copy()\n",
    "    train_df[target_col] = y_train.values\n",
    "    train_df.to_csv(train_data.path + \".csv\", index=False)\n",
    "    logging.info(\"Train data saved\")\n",
    "    \n",
    "    # 7. Save test data\n",
    "    test_df = X_test_scaled.copy()\n",
    "    test_df[target_col] = y_test.values\n",
    "    test_df.to_csv(test_data.path + \".csv\", index=False)\n",
    "    logging.info(\"Test data saved\")\n",
    "    \n",
    "    # 8. Save scaler\n",
    "    scaler_out.metadata[\"file_type\"] = \".pkl\"\n",
    "    scaler_out.metadata[\"algo\"] = \"scaler\"\n",
    "    joblib.dump(scaler, scaler_out.path + \".pkl\")\n",
    "    logging.info(\"Scaler saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3494d2-bea7-415f-9832-fcf1f2c9fe4a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aac0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_rf(\n",
    "    train_data: Input[Dataset],\n",
    "    model_out: Output[Model]\n",
    ") -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Train Random Forest model'''\n",
    "    import pandas as pd\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    import joblib\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load train data\n",
    "    df_train = pd.read_csv(train_data.path + \".csv\")\n",
    "    X_train = df_train.drop(columns=[\"is_churned\"])\n",
    "    y_train = df_train[\"is_churned\"]\n",
    "    \n",
    "    logging.info(f\"Training RF on {X_train.shape[0]} samples\")\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on training data (for comparison)\n",
    "    y_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred)\n",
    "    train_f1 = f1_score(y_train, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"train_accuracy\": float(train_accuracy),\n",
    "        \"train_f1_score\": float(train_f1)\n",
    "    }\n",
    "    logging.info(f\"RF Training Metrics: {metrics_dict}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_out.metadata[\"file_type\"] = \".pkl\"\n",
    "    model_out.metadata[\"algo\"] = \"best_model\"\n",
    "    joblib.dump(model, model_out.path + \".pkl\")\n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911d312-549c-4be7-bef0-e02c9d8cf80f",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Training LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def train_lr(\n",
    "    train_data: Input[Dataset],\n",
    "    model_out: Output[Model]\n",
    ") -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Train Logistic Regression model'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "    import joblib\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load train data\n",
    "    df_train = pd.read_csv(train_data.path + \".csv\")\n",
    "    X_train = df_train.drop(columns=[\"is_churned\"])\n",
    "    y_train = df_train[\"is_churned\"]\n",
    "    \n",
    "    logging.info(f\"Training LR on {X_train.shape[0]} samples\")\n",
    "    \n",
    "    # Train model\n",
    "    model = LogisticRegression(max_iter=500, class_weight=\"balanced\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on training data (for comparison)\n",
    "    y_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_pred)\n",
    "    train_f1 = f1_score(y_train, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"train_accuracy\": float(train_accuracy),\n",
    "        \"train_f1_score\": float(train_f1)\n",
    "    }\n",
    "    logging.info(f\"LR Training Metrics: {metrics_dict}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_out.metadata[\"file_type\"] = \".pkl\"\n",
    "    model_out.metadata[\"algo\"] = \"best_model\"\n",
    "    joblib.dump(model, model_out.path + \".pkl\")\n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33b9f62-14d3-4b97-bd9a-f15b3d47758a",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1811937a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_rf(\n",
    "    model: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    predictions: Output[Dataset]\n",
    ") -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Make predictions with RF and evaluate'''\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load test data\n",
    "    df_test = pd.read_csv(test_data.path + \".csv\")\n",
    "    X_test = df_test.drop(columns=[\"is_churned\"])\n",
    "    y_test = df_test[\"is_churned\"]\n",
    "    \n",
    "    # Load model\n",
    "    model_rf = joblib.load(model.path + \".pkl\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model_rf.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_f1_score\": float(f1)\n",
    "    }\n",
    "    logging.info(f\"RF Test Metrics: {metrics_dict}\")\n",
    "    logging.info(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    result_df = df_test.copy()\n",
    "    result_df['prediction'] = y_pred\n",
    "    result_df.to_csv(predictions.path + \".csv\", index=False)\n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4e9c1-a1d4-412f-8533-8e741fd3f79c",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Prediction-LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dee5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def predict_lr(\n",
    "    model: Input[Model],\n",
    "    test_data: Input[Dataset],\n",
    "    predictions: Output[Dataset]\n",
    ") -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Make predictions with LR and evaluate'''\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load test data\n",
    "    df_test = pd.read_csv(test_data.path + \".csv\")\n",
    "    X_test = df_test.drop(columns=[\"is_churned\"])\n",
    "    y_test = df_test[\"is_churned\"]\n",
    "    \n",
    "    # Load model\n",
    "    model_lr = joblib.load(model.path + \".pkl\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = model_lr.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_f1_score\": float(f1)\n",
    "    }\n",
    "    logging.info(f\"LR Test Metrics: {metrics_dict}\")\n",
    "    logging.info(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    # Save predictions\n",
    "    result_df = df_test.copy()\n",
    "    result_df['prediction'] = y_pred\n",
    "    result_df.to_csv(predictions.path + \".csv\", index=False)\n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c183e7-f9dd-48d7-b37e-69646bb08203",
   "metadata": {},
   "source": [
    "#### Pipeline Component : Algorithm Selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792cdc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_models(rf_metrics: dict, lr_metrics: dict) -> str:\n",
    "    '''Compare models and select winner'''\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    logging.info(f\"RF Metrics: {rf_metrics}\")\n",
    "    logging.info(f\"LR Metrics: {lr_metrics}\")\n",
    "    \n",
    "    # Compare based on test F1-score\n",
    "    rf_f1 = rf_metrics.get(\"test_f1_score\", 0)\n",
    "    lr_f1 = lr_metrics.get(\"test_f1_score\", 0)\n",
    "    \n",
    "    if rf_f1 > lr_f1:\n",
    "        logging.info(f\"Winner: Random Forest (F1={rf_f1:.3f})\")\n",
    "        return \"RF\"\n",
    "    else:\n",
    "        logging.info(f\"Winner: Logistic Regression (F1={lr_f1:.3f})\")\n",
    "        return \"LR\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d160e07a",
   "metadata": {},
   "source": [
    "### Download Production Model from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9171f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def download_production_model(\n",
    "    project_id: str, \n",
    "    bucket_name: str,\n",
    "    production_model: Output[Model]\n",
    ") -> str:\n",
    "    '''Download current production model from GCS (if exists)'''\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    # Check if production model exists\n",
    "    blob = bucket.blob(\"best_model.pkl\")\n",
    "    \n",
    "    if not blob.exists():\n",
    "        logging.warning(\"No production model found. This is the first deployment.\")\n",
    "        return \"NO_BASELINE\"\n",
    "    \n",
    "    # Download production model\n",
    "    blob.download_to_filename(production_model.path + \".pkl\")\n",
    "    production_model.metadata[\"file_type\"] = \".pkl\"\n",
    "    production_model.metadata[\"algo\"] = \"production_model\"\n",
    "    \n",
    "    logging.info(f\"âœ… Downloaded production model from gs://{bucket_name}/best_model.pkl\")\n",
    "    return \"BASELINE_EXISTS\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fd03f",
   "metadata": {},
   "source": [
    "### Evaluate Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=['pandas', 'scikit-learn==1.3.2', 'joblib'],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def evaluate_production_model(\n",
    "    model: Input[Model],\n",
    "    test_data: Input[Dataset]\n",
    ") -> NamedTuple('outputs', metrics=dict):\n",
    "    '''Evaluate production model on test set'''\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Load test data\n",
    "    df_test = pd.read_csv(test_data.path + \".csv\")\n",
    "    X_test = df_test.drop(columns=[\"is_churned\"])\n",
    "    y_test = df_test[\"is_churned\"]\n",
    "    \n",
    "    # Load model\n",
    "    production_model = joblib.load(model.path + \".pkl\")\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = production_model.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_dict = {\n",
    "        \"test_accuracy\": float(accuracy),\n",
    "        \"test_f1_score\": float(f1)\n",
    "    }\n",
    "    logging.info(f\"Production Model Test Metrics: {metrics_dict}\")\n",
    "    logging.info(f\"\\n{classification_report(y_test, y_pred)}\")\n",
    "    \n",
    "    outputs = NamedTuple('outputs', metrics=dict)\n",
    "    return outputs(metrics_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2bb4e8",
   "metadata": {},
   "source": [
    "### Compare Winner vs Production Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b07c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def compare_with_baseline(\n",
    "    new_model_metrics: dict,\n",
    "    production_metrics: dict,\n",
    "    baseline_status: str,\n",
    "    metric_name: str = \"test_f1_score\"\n",
    ") -> str:\n",
    "    '''\n",
    "    Compare new model against production baseline.\n",
    "    Returns \"DEPLOY\" or \"REJECT\"\n",
    "    '''\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # If no baseline exists, deploy new model\n",
    "    if baseline_status == \"NO_BASELINE\":\n",
    "        logging.info(\"DECISION: DEPLOY - No baseline model exists (first deployment)\")\n",
    "        return \"DEPLOY\"\n",
    "    \n",
    "    # Extract scores\n",
    "    production_score = production_metrics.get(metric_name, 0)\n",
    "    new_score = new_model_metrics.get(metric_name, 0)\n",
    "    \n",
    "    improvement = new_score - production_score\n",
    "    improvement_pct = (improvement / production_score * 100) if production_score > 0 else 0\n",
    "    \n",
    "    # Minimum acceptable: 98% of production (allow 2% degradation)\n",
    "    min_acceptable = 0.98 * production_score\n",
    "    \n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(\"BASELINE COMPARISON\")\n",
    "    logging.info(\"=\" * 60)\n",
    "    logging.info(f\"Production Model {metric_name}: {production_score:.4f}\")\n",
    "    logging.info(f\"New Model {metric_name}: {new_score:.4f}\")\n",
    "    logging.info(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "    logging.info(\"=\" * 60)\n",
    "    \n",
    "    # Decision: new model must be >= production model\n",
    "    if new_score >= min_acceptable:\n",
    "        logging.info(\"DECISION: DEPLOY\")\n",
    "        logging.info(\"   New model performs equal to or better than production\")\n",
    "        return \"DEPLOY\"\n",
    "    else:\n",
    "        logging.warning(\"DECISION: REJECT\")\n",
    "        logging.warning(\"   New model worse than production baseline\")\n",
    "        logging.warning(f\"   Required: >= {production_score:.4f}\")\n",
    "        logging.warning(f\"   Achieved: {new_score:.4f}\")\n",
    "        return \"REJECT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ed599-9cd0-4215-a17d-1a8baaa2c76c",
   "metadata": {},
   "source": [
    "### Upload Model and Metrics to Google Bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a940ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-storage\"],\n",
    "    base_image=\"python:3.10.7-slim\"\n",
    ")\n",
    "def upload_to_gcs(project_id: str, bucket_name: str, artifact: Input[Model]):\n",
    "    '''Upload artifact to GCS'''\n",
    "    from google.cloud import storage\n",
    "    import logging\n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    \n",
    "    file_name = artifact.metadata[\"algo\"] + artifact.metadata[\"file_type\"]\n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.upload_from_filename(artifact.path + artifact.metadata[\"file_type\"])\n",
    "    \n",
    "    logging.info(f\"Uploaded {file_name} to gs://{bucket_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166590b3-f788-4e4c-8e31-fb981da56966",
   "metadata": {},
   "source": [
    "#### Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3ee670",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=\"spotify-churn-training-pipeline\")\n",
    "def pipeline(\n",
    "    project_id: str,\n",
    "    data_bucket: str,\n",
    "    dataset_filename: str,\n",
    "    model_repo: str\n",
    "):\n",
    "    # Step 1: Download raw data\n",
    "    download_op = download_data(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=dataset_filename\n",
    "    )\n",
    "    \n",
    "    # Step 2: Preprocess and split data\n",
    "    split_op = preprocess_and_split(\n",
    "        raw_data=download_op.outputs[\"dataset\"]\n",
    "    )\n",
    "    \n",
    "    # Step 3: Train both models in parallel\n",
    "    train_rf_op = train_rf(\n",
    "        train_data=split_op.outputs[\"train_data\"]\n",
    "    )\n",
    "    \n",
    "    train_lr_op = train_lr(\n",
    "        train_data=split_op.outputs[\"train_data\"]\n",
    "    )\n",
    "    \n",
    "    # Step 4: Predict with both models on test data\n",
    "    predict_rf_op = predict_rf(\n",
    "        model=train_rf_op.outputs[\"model_out\"],\n",
    "        test_data=split_op.outputs[\"test_data\"]\n",
    "    )\n",
    "    \n",
    "    predict_lr_op = predict_lr(\n",
    "        model=train_lr_op.outputs[\"model_out\"],\n",
    "        test_data=split_op.outputs[\"test_data\"]\n",
    "    )\n",
    "    \n",
    "    # Step 5: Compare RF vs LR models based on test performance\n",
    "    compare_op = compare_models(\n",
    "        rf_metrics=predict_rf_op.outputs[\"metrics\"],\n",
    "        lr_metrics=predict_lr_op.outputs[\"metrics\"]\n",
    "    ).after(predict_rf_op, predict_lr_op)\n",
    "    \n",
    "    # Step 6: Download current production model (if exists)\n",
    "    download_prod_op = download_production_model(\n",
    "        project_id=project_id,\n",
    "        bucket_name=model_repo\n",
    "    ).after(compare_op)\n",
    "    \n",
    "    # Step 7: Evaluate production model on test set (only if baseline exists)\n",
    "    with dsl.If(download_prod_op.outputs[\"Output\"] == \"BASELINE_EXISTS\"):\n",
    "        eval_prod_rf_op = evaluate_production_model(\n",
    "            model=download_prod_op.outputs[\"production_model\"],\n",
    "            test_data=split_op.outputs[\"test_data\"]\n",
    "        )\n",
    "    \n",
    "    # Step 8a: Compare winner (RF) against baseline\n",
    "    with dsl.If(compare_op.output == \"RF\"):\n",
    "        # If baseline exists, compare; otherwise get dummy metrics\n",
    "        with dsl.If(download_prod_op.outputs[\"Output\"] == \"BASELINE_EXISTS\"):\n",
    "            baseline_compare_rf = compare_with_baseline(\n",
    "                new_model_metrics=predict_rf_op.outputs[\"metrics\"],\n",
    "                production_metrics=eval_prod_rf_op.outputs[\"metrics\"],\n",
    "                baseline_status=download_prod_op.outputs[\"Output\"]\n",
    "            )\n",
    "        with dsl.Else():\n",
    "            baseline_compare_rf = compare_with_baseline(\n",
    "                new_model_metrics=predict_rf_op.outputs[\"metrics\"],\n",
    "                production_metrics={\"test_f1_score\": 0.0},  # Dummy\n",
    "                baseline_status=\"NO_BASELINE\"\n",
    "            )\n",
    "        \n",
    "        # Upload RF model only if approved\n",
    "        with dsl.If(baseline_compare_rf.output == \"DEPLOY\"):\n",
    "            upload_to_gcs(\n",
    "                project_id=project_id,\n",
    "                bucket_name=model_repo,\n",
    "                artifact=train_rf_op.outputs[\"model_out\"]\n",
    "            )\n",
    "            upload_to_gcs(\n",
    "                project_id=project_id,\n",
    "                bucket_name=model_repo,\n",
    "                artifact=split_op.outputs[\"scaler_out\"]\n",
    "            )\n",
    "    \n",
    "    # Step 8b: Compare winner (LR) against baseline\n",
    "    with dsl.If(compare_op.output == \"LR\"):\n",
    "        # If baseline exists, compare; otherwise get dummy metrics\n",
    "        with dsl.If(download_prod_op.outputs[\"Output\"] == \"BASELINE_EXISTS\"):\n",
    "            baseline_compare_lr = compare_with_baseline(\n",
    "                new_model_metrics=predict_lr_op.outputs[\"metrics\"],\n",
    "                production_metrics=eval_prod_rf_op.outputs[\"metrics\"],\n",
    "                baseline_status=download_prod_op.outputs[\"Output\"]\n",
    "            )\n",
    "        with dsl.Else():\n",
    "            baseline_compare_lr = compare_with_baseline(\n",
    "                new_model_metrics=predict_lr_op.outputs[\"metrics\"],\n",
    "                production_metrics={\"test_f1_score\": 0.0},  # Dummy\n",
    "                baseline_status=\"NO_BASELINE\"\n",
    "            )\n",
    "        \n",
    "        # Upload LR model only if approved\n",
    "        with dsl.If(baseline_compare_lr.output == \"DEPLOY\"):\n",
    "            upload_to_gcs(\n",
    "                project_id=project_id,\n",
    "                bucket_name=model_repo,\n",
    "                artifact=train_lr_op.outputs[\"model_out\"]\n",
    "            )\n",
    "            upload_to_gcs(\n",
    "                project_id=project_id,\n",
    "                bucket_name=model_repo,\n",
    "                artifact=split_op.outputs[\"scaler_out\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac278200-c580-4f40-bc8b-1817d3b13c13",
   "metadata": {},
   "source": [
    "#### Compile the pipeline into a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f669daa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile\n",
    "from kfp import compiler\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='spotify_churn_pipeline.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87025e-08d7-4608-b37d-c929b6eb5a3c",
   "metadata": {},
   "source": [
    "#### Submit the pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceb39c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "aip.init(\n",
    "    project=PROJECT_ID,\n",
    "    staging_bucket=PIPELINE_ROOT,\n",
    "    location=REGION\n",
    ")\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"spotify-churn-pipeline\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"spotify_churn_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    location=REGION,\n",
    "    parameter_values={\n",
    "        'project_id': PROJECT_ID,\n",
    "        'data_bucket': 'data_spotify_2038050',\n",
    "        'dataset_filename': 'spotify_churn_dataset.csv',\n",
    "        'model_repo': 'models_spotify_2038050'\n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
