{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b35c21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions and install if needed\n",
    "#!pip install google-cloud-aiplatform==1.66.0 kfp==2.6.0 scikit-learn==1.4.2 pandas==2.2.2 joblib==1.4.2 --quiet\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "import json\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7859fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# ---- GCP and bucket setup ----\n",
    "PROJECT_ID = \"de2025-471807\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# 3 buckets for clear separation of stages\n",
    "DATA_BUCKET = \"gs://spotify_data_25\"\n",
    "MODEL_BUCKET = \"gs://spotify_models\"\n",
    "TEMP_BUCKET  = \"gs://spotify_temp\"\n",
    "\n",
    "# Paths used in the pipeline\n",
    "DATA_URI      = f\"{DATA_BUCKET}/spotify_churn_dataset.csv\"\n",
    "PIPELINE_ROOT = f\"{TEMP_BUCKET}/pipeline_root\"\n",
    "PIPELINE_NAME = \"spotify_churn_pipeline_lr\"\n",
    "\n",
    "# Features and target (numeric only for now)\n",
    "NUMERIC_FEATURES = [\"age\",\"listening_time\",\"songs_played_per_day\",\"skip_rate\",\"ads_listened_per_week\"]\n",
    "TARGET_COL = \"is_churned\"\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c53cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\",\"gcsfs\"])\n",
    "def data_ingestion(gcs_csv_uri: str, dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(gcs_csv_uri)\n",
    "    print(\"✅ Data loaded:\", df.shape)\n",
    "    df.to_csv(dataset.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f19ac673",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\",\"scikit-learn==1.4.2\",\"joblib\"])\n",
    "def train_logistic_regression(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    numeric_features_json: str,\n",
    "    target_col: str = \"is_churned\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    import json, os, joblib, pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "    NUMERIC_FEATURES = json.loads(numeric_features_json)\n",
    "    df = pd.read_csv(dataset.path)\n",
    "\n",
    "    X = df[NUMERIC_FEATURES]\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=200, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    metrics.log_metric(\"accuracy\",  float(acc))\n",
    "    metrics.log_metric(\"roc_auc\",   float(auc))\n",
    "    metrics.log_metric(\"precision\", float(precision))\n",
    "    metrics.log_metric(\"recall\",    float(recall))\n",
    "    metrics.log_metric(\"f1\",        float(f1))\n",
    "\n",
    "    print(\"✅ Model Performance:\")\n",
    "    print(json.dumps({\"accuracy\":acc,\"roc_auc\":auc,\"precision\":precision,\"recall\":recall,\"f1\":f1},indent=2))\n",
    "\n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    joblib.dump((scaler, model), os.path.join(model_artifact.path, \"model.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12a5bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\",\"gcsfs\"])\n",
    "def register_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_bucket: str,\n",
    "    model_artifact: Input[Model],\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import gcsfs, os\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "    gcs_model_path = model_bucket.rstrip(\"/\") + \"/spotify_lr_model/model.pkl\"\n",
    "    local_model = os.path.join(model_artifact.path, \"model.pkl\")\n",
    "\n",
    "    # Upload model to your dedicated model bucket\n",
    "    with fs.open(gcs_model_path, \"wb\") as f_out, open(local_model, \"rb\") as f_in:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "    # Register in Vertex Model Registry\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=\"spotify-churn-lr\",\n",
    "        artifact_uri=model_bucket,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-4:latest\",\n",
    "        description=\"Spotify churn Logistic Regression numeric-only model\",\n",
    "    )\n",
    "\n",
    "    print(\"✅ Model registered in Vertex AI:\", model.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d21069ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"Spotify Churn Prediction Pipeline (Logistic Regression, numeric-only)\"\n",
    ")\n",
    "def spotify_churn_pipeline(\n",
    "    gcs_csv_uri: str = DATA_URI,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    model_bucket: str = MODEL_BUCKET,\n",
    "):\n",
    "    ingest = data_ingestion(gcs_csv_uri=gcs_csv_uri)\n",
    "\n",
    "    train = train_logistic_regression(\n",
    "        dataset=ingest.outputs[\"dataset\"],\n",
    "        numeric_features_json=json.dumps(NUMERIC_FEATURES),\n",
    "        target_col=TARGET_COL,\n",
    "    )\n",
    "\n",
    "    register = register_model(\n",
    "        project=project,\n",
    "        region=region,\n",
    "        model_bucket=model_bucket,\n",
    "        model_artifact=train.outputs[\"model_artifact\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "916a54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline compiled: pipeline/build/spotify_churn_pipeline.json\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "os.makedirs(\"pipeline/build\", exist_ok=True)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=spotify_churn_pipeline,\n",
    "    package_path=\"pipeline/build/spotify_churn_pipeline.json\",\n",
    ")\n",
    "print(\"✅ Pipeline compiled: pipeline/build/spotify_churn_pipeline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "305a412c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/spotify-churn-pipeline-lr-20251016163743?project=951195898169\n",
      "PipelineJob projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743 current state:\n",
      "3\n",
      "PipelineJob projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743 current state:\n",
      "3\n",
      "PipelineJob projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743 current state:\n",
      "3\n",
      "PipelineJob projects/951195898169/locations/us-central1/pipelineJobs/spotify-churn-pipeline-lr-20251016163743 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [data-ingestion].; Job (project_id = de2025-471807, job_id = 4519334325958737920) is failed due to the above error.; Failed to handle the job: {project_number = 951195898169, job_id = 4519334325958737920}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Create and run pipeline job\u001b[39;00m\n\u001b[32m      7\u001b[39m job = aiplatform.PipelineJob(\n\u001b[32m      8\u001b[39m     display_name=\u001b[33m\"\u001b[39m\u001b[33mspotify-churn-lr-pipeline\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     template_path=\u001b[33m\"\u001b[39m\u001b[33mpipeline/build/spotify_churn_pipeline.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     pipeline_root=PIPELINE_ROOT,\n\u001b[32m     11\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mjob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE25-MLOps-group5\\.venv\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:334\u001b[39m, in \u001b[36mPipelineJob.run\u001b[39m\u001b[34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[32m    311\u001b[39m \n\u001b[32m    312\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    330\u001b[39m \u001b[33;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    332\u001b[39m network = network \u001b[38;5;129;01mor\u001b[39;00m initializer.global_config.network\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m=\u001b[49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_preflight_validations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE25-MLOps-group5\\.venv\\Lib\\site-packages\\google\\cloud\\aiplatform\\base.py:863\u001b[39m, in \u001b[36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m    862\u001b[39m         VertexAiResourceNounWithFutureManager.wait(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m863\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[32m    866\u001b[39m internal_callbacks = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE25-MLOps-group5\\.venv\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:382\u001b[39m, in \u001b[36mPipelineJob._run\u001b[39m\u001b[34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout, enable_preflight_validations)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[33;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[32m    355\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m \u001b[33;03m        Optional. Whether to enable preflight validations for the PipelineJob.\u001b[39;00m\n\u001b[32m    373\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;28mself\u001b[39m.submit(\n\u001b[32m    375\u001b[39m     service_account=service_account,\n\u001b[32m    376\u001b[39m     network=network,\n\u001b[32m   (...)\u001b[39m\u001b[32m    379\u001b[39m     enable_preflight_validations=enable_preflight_validations,\n\u001b[32m    380\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;66;03m# AutoSxS view model evaluations\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m details \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.task_details:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rderi\\OneDrive\\Bureaublad\\Jads 2025-2027\\Data Engineering\\Data Engineering Assignment\\DE25-MLOps-group5\\.venv\\Lib\\site-packages\\google\\cloud\\aiplatform\\pipeline_jobs.py:793\u001b[39m, in \u001b[36mPipelineJob._block_until_complete\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[32m    791\u001b[39m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[32m    792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gca_resource.state \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % \u001b[38;5;28mself\u001b[39m._gca_resource.error)\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    795\u001b[39m     _LOGGER.log_action_completed_against_resource(\u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcompleted\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Job failed with:\ncode: 9\nmessage: \" The DAG failed because some tasks failed. The failed tasks are: [data-ingestion].; Job (project_id = de2025-471807, job_id = 4519334325958737920) is failed due to the above error.; Failed to handle the job: {project_number = 951195898169, job_id = 4519334325958737920}\"\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI client with your temporary/staging bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=TEMP_BUCKET)\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"spotify-churn-lr-pipeline\",\n",
    "    template_path=\"pipeline/build/spotify_churn_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
