{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b35c21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions and install if needed\n",
    "#!pip install google-cloud-aiplatform==1.66.0 kfp==2.6.0 scikit-learn==1.4.2 pandas==2.2.2 joblib==1.4.2 --quiet\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "import json\n",
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model, Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7859fc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n"
     ]
    }
   ],
   "source": [
    "# ---- GCP and bucket setup ----\n",
    "PROJECT_ID = \"de2025-471807\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "# 3 buckets for clear separation of stages\n",
    "DATA_BUCKET = \"gs://spotify_data_25\"\n",
    "MODEL_BUCKET = \"gs://spotify_models\"\n",
    "TEMP_BUCKET  = \"gs://spotify_temp\"\n",
    "\n",
    "# Paths used in the pipeline\n",
    "DATA_URI      = f\"{DATA_BUCKET}/spotify_churn_dataset.csv\"\n",
    "PIPELINE_ROOT = f\"{TEMP_BUCKET}/pipeline_root\"\n",
    "PIPELINE_NAME = \"spotify_churn_pipeline_lr\"\n",
    "\n",
    "# Features and target (numeric only for now)\n",
    "NUMERIC_FEATURES = [\"age\",\"listening_time\",\"songs_played_per_day\",\"skip_rate\",\"ads_listened_per_week\"]\n",
    "TARGET_COL = \"is_churned\"\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c53cabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\",\"gcsfs\"])\n",
    "def data_ingestion(gcs_csv_uri: str, dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(gcs_csv_uri)\n",
    "    print(\"✅ Data loaded:\", df.shape)\n",
    "    df.to_csv(dataset.path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f19ac673",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"pandas\",\"scikit-learn==1.4.2\",\"joblib\"])\n",
    "def train_logistic_regression(\n",
    "    dataset: Input[Dataset],\n",
    "    model_artifact: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "    numeric_features_json: str,\n",
    "    target_col: str = \"is_churned\",\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    import json, os, joblib, pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import accuracy_score, roc_auc_score, precision_recall_fscore_support\n",
    "\n",
    "    NUMERIC_FEATURES = json.loads(numeric_features_json)\n",
    "    df = pd.read_csv(dataset.path)\n",
    "\n",
    "    X = df[NUMERIC_FEATURES]\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=200, class_weight=\"balanced\", solver=\"liblinear\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    y_prob = model.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    metrics.log_metric(\"accuracy\",  float(acc))\n",
    "    metrics.log_metric(\"roc_auc\",   float(auc))\n",
    "    metrics.log_metric(\"precision\", float(precision))\n",
    "    metrics.log_metric(\"recall\",    float(recall))\n",
    "    metrics.log_metric(\"f1\",        float(f1))\n",
    "\n",
    "    print(\"✅ Model Performance:\")\n",
    "    print(json.dumps({\"accuracy\":acc,\"roc_auc\":auc,\"precision\":precision,\"recall\":recall,\"f1\":f1},indent=2))\n",
    "\n",
    "    os.makedirs(model_artifact.path, exist_ok=True)\n",
    "    joblib.dump((scaler, model), os.path.join(model_artifact.path, \"model.pkl\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12a5bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=[\"google-cloud-aiplatform\",\"gcsfs\"])\n",
    "def register_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_bucket: str,\n",
    "    model_artifact: Input[Model],\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    import gcsfs, os\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "    gcs_model_path = model_bucket.rstrip(\"/\") + \"/spotify_lr_model/model.pkl\"\n",
    "    local_model = os.path.join(model_artifact.path, \"model.pkl\")\n",
    "\n",
    "    # Upload model to your dedicated model bucket\n",
    "    with fs.open(gcs_model_path, \"wb\") as f_out, open(local_model, \"rb\") as f_in:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "    # Register in Vertex Model Registry\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=\"spotify-churn-lr\",\n",
    "        artifact_uri=model_bucket,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-4:latest\",\n",
    "        description=\"Spotify churn Logistic Regression numeric-only model\",\n",
    "    )\n",
    "\n",
    "    print(\"✅ Model registered in Vertex AI:\", model.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d21069ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    description=\"Spotify Churn Prediction Pipeline (Logistic Regression, numeric-only)\"\n",
    ")\n",
    "def spotify_churn_pipeline(\n",
    "    gcs_csv_uri: str = DATA_URI,\n",
    "    project: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    model_bucket: str = MODEL_BUCKET,\n",
    "):\n",
    "    ingest = data_ingestion(gcs_csv_uri=gcs_csv_uri)\n",
    "\n",
    "    train = train_logistic_regression(\n",
    "        dataset=ingest.outputs[\"dataset\"],\n",
    "        numeric_features_json=json.dumps(NUMERIC_FEATURES),\n",
    "        target_col=TARGET_COL,\n",
    "    )\n",
    "\n",
    "    register = register_model(\n",
    "        project=project,\n",
    "        region=region,\n",
    "        model_bucket=model_bucket,\n",
    "        model_artifact=train.outputs[\"model_artifact\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "916a54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline compiled: pipeline/build/spotify_churn_pipeline.json\n"
     ]
    }
   ],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "os.makedirs(\"pipeline/build\", exist_ok=True)\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=spotify_churn_pipeline,\n",
    "    package_path=\"pipeline/build/spotify_churn_pipeline.json\",\n",
    ")\n",
    "print(\"✅ Pipeline compiled: pipeline/build/spotify_churn_pipeline.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "305a412c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'aiplatform' from 'google.cloud' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m aiplatform\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize Vertex AI client with your temporary/staging bucket\u001b[39;00m\n\u001b[32m      4\u001b[39m aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=TEMP_BUCKET)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'aiplatform' from 'google.cloud' (unknown location)"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI client with your temporary/staging bucket\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=TEMP_BUCKET)\n",
    "\n",
    "# Create and run pipeline job\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"spotify-churn-lr-pipeline\",\n",
    "    template_path=\"pipeline/build/spotify_churn_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
